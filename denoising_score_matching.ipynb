{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9147d9-2432-43eb-a291-2d03e8da4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterator, Sequence\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import haiku as hk\n",
    "import chex\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbbfd5e-de4a-4eea-9110-ef0cc0d0c80e",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a282b-bfd1-42b3-a89e-e9ecfb71eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checkerboard(*, num: int, rng: np.random.Generator) -> np.ndarray:\n",
    "    # https://github.com/malbergo/stochastic-interpolants/blob/main/notebooks/checker-mirror.ipynb\n",
    "    x1 = rng.uniform(size=num) * 4 - 2\n",
    "    x2_ = rng.uniform(size=num) - rng.choice([0, 1], size=(num,)) * 2\n",
    "    x2 = x2_ + (np.floor(x1) % 2)\n",
    "    x = np.hstack([x1[:, None], x2[:, None]]) * 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6d989-eb03-49ef-bc56-c47577580046",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = generate_checkerboard(num=2000, rng=np.random.default_rng())\n",
    "plt.plot(X_train[:, 0], X_train[:, 1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc63e9-9a59-4f79-8f55-0b4465ed1ff2",
   "metadata": {},
   "source": [
    "## Define MLP score function\n",
    "\n",
    "Here, we use the `flax` [library](https://flax.readthedocs.io/en/latest/) to define a fully connected neural network that represents our score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5275295-390b-4545-b8f4-222e473b3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation = Callable[[jax.Array], jax.Array]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    features: Sequence[int]  # Number of neurons in each layer\n",
    "    activation: Activation = nn.swish\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        for f in self.features[:-1]:\n",
    "            x = nn.Dense(f)(x)\n",
    "            x = self.activation(x)\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(features=[64, 64, 2])\n",
    "print(model.tabulate(jax.random.PRNGKey(0), np.zeros((1, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6f3f6-6de7-4543-8979-b8f124f9dba7",
   "metadata": {},
   "source": [
    "## Define the score matching loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8b4976-d704-4e90-b8b9-47c86bd2b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss is not divided by n?\n",
    "# Investigate whether alway size n for last batch\n",
    "def denoising_score_matching_loss(params: chex.ArrayTree, batch: jax.Array, key: chex.PRNGKey, std: float, k: int) -> float:\n",
    "    n, m = batch.shape\n",
    "    batch = jnp.tile(batch[None, :, :], (k, 1, 1))\n",
    "\n",
    "    # Apply noise to datapoints\n",
    "    noise = jax.random.normal(key, batch.shape) * std\n",
    "    noised_batch = noise + batch\n",
    "\n",
    "    # DSM loss: x_tilde + sigma^2*f(x_tilde) - x\n",
    "    fs = model.apply(params, noised_batch.reshape(k * n, m)).reshape(k, n, m)\n",
    "    loss = jnp.sum(jnp.square(noised_batch + std**2 * fs - batch)) / k\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80891322-1050-4d88-8535-537af639b635",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418baf92-e4bd-4140-a90b-5c5d1a5e6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=1e-1)\n",
    "prng_seq = hk.PRNGSequence(jax.random.PRNGKey(0))\n",
    "\n",
    "@jax.jit\n",
    "def do_batch_update(\n",
    "    batch: jax.Array,\n",
    "    params: chex.ArrayTree, \n",
    "    opt_state: optax.OptState,  # Optimizer state\n",
    "    key: chex.PRNGKey,\n",
    ") -> tuple[float, chex.ArrayTree, optax.OptState]:\n",
    "    loss, grad = jax.value_and_grad(denoising_score_matching_loss)(params, batch, key, std=0.01, k=100)\n",
    "    updates, opt_state = optimizer.update(grad, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, params, opt_state\n",
    "\n",
    "\n",
    "class BatchManager(Iterator[np.ndarray]):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        batch_size: int,\n",
    "        key: chex.PRNGKey\n",
    "    ):\n",
    "        batch_size = min(batch_size, len(data))\n",
    "        self._num_batches = len(data) // batch_size\n",
    "        self._batch_idx = None\n",
    "        self._batch_size = batch_size\n",
    "        self._key = hk.PRNGSequence(key)\n",
    "        self._data = data\n",
    "        self._reset()\n",
    "\n",
    "    @property\n",
    "    def num_batches(self) -> int:\n",
    "        return self._num_batches\n",
    "\n",
    "    def _reset(self) -> None:\n",
    "        self._perm = np.array(jax.random.permutation(next(self._key), np.arange(len(self._data))))\n",
    "        self._batch_idx = 0\n",
    "\n",
    "    def __next__(self) -> np.ndarray:\n",
    "        assert self._batch_idx is not None\n",
    "        assert self._batch_idx >= 0 and self._batch_idx < self._num_batches\n",
    "        inds = self._perm[self._batch_idx * self._batch_size : (self._batch_idx + 1) * self._batch_size]\n",
    "        batch = self._data[inds]\n",
    "        self._batch_idx += 1\n",
    "        if self._batch_idx >= self._num_batches:\n",
    "            self._reset()\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1027f-875e-4e28-9f20-7eed921ad9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(next(prng_seq), X_train[:1, ...])\n",
    "opt_state = optimizer.init(params)\n",
    "bm = BatchManager(data=X_train, batch_size=128, key=next(prng_seq))\n",
    "\n",
    "losses = []\n",
    "for _ in tqdm.tqdm(range(10000)):\n",
    "    batch = next(bm)\n",
    "    loss, params, opt_state = do_batch_update(batch, params, opt_state, key=next(prng_seq))\n",
    "    losses.append(loss)\n",
    "losses = np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca507c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1000\n",
    "window = np.ones(window_size) / window_size\n",
    "ave_losses = np.convolve(losses, window, mode='valid')\n",
    "plt.plot(ave_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfe699-3cfa-431a-9d7c-a03362201af5",
   "metadata": {},
   "source": [
    "## Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811dc95-dc0a-4058-ba20-7b284f1d6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.partial(jax.jit, static_argnames=(\"num_steps\",))\n",
    "def langevin_sampling(\n",
    "    params: chex.ArrayTree,\n",
    "    key: chex.PRNGKey,\n",
    "    step_size: float,\n",
    "    initial_samples: jax.Array,\n",
    "    num_steps: int,\n",
    ") -> jax.Array:\n",
    "\n",
    "    def scan_fn(carry, _):\n",
    "        states, key = carry\n",
    "        key, sk = jax.random.split(key)\n",
    "        noise = jax.random.normal(sk, shape=states.shape)\n",
    "        next_states = states + step_size * model.apply(params, states) + jnp.sqrt(2 * step_size) * noise\n",
    "        return (next_states, key), None\n",
    "\n",
    "    states = initial_samples\n",
    "    (states, _), _ = jax.lax.scan(scan_fn, (states, key), jnp.arange(num_steps))\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c06451-8ac8-4fbf-ad03-6d361d1054f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = langevin_sampling(\n",
    "    params,\n",
    "    next(prng_seq),\n",
    "    5e-3,\n",
    "    2 * jax.random.normal(next(prng_seq), shape=(2000, 2)),\n",
    "    1000)\n",
    "plt.plot(samples[:, 0], samples[:, 1], '.')\n",
    "plt.plot(X_train[:, 0], X_train[:, 1], 'o', alpha=0.2)\n",
    "\n",
    "plt.xlim([-4, 4])\n",
    "plt.ylim([-4, 4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
